# -*- coding: utf-8 -*-
"""Raw model comparison 4o.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R38mQ429AHiyqwlbzuUVsb-nDF0QeLdK
"""

! pip install llama_index

! pip install ragas==0.1.21

import os
from llama_index.core import SimpleDirectoryReader
from llama_index.core import VectorStoreIndex
from llama_index.core.node_parser import TokenTextSplitter
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings
from ragas.testset.generator import TestsetGenerator
from ragas.testset.evolutions import simple, reasoning, multi_context
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from ragas.integrations.llama_index import evaluate



# Get the API key from the environment
os.environ["OPENAI_API_KEY"] = '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'


# For different LLM models
RAG_LLM = "gpt-4o"
GENERATOR_LLM = "gpt-3.5-turbo"
CRITIC_LLM = "gpt-3.5-turbo"
EVALUATOR_LLM = "gpt-3.5-turbo"

# For different temperatures
TEMPERATURE = 0.1

# For different embeddings
RAG_EMBEDDING = "text-embedding-ada-002"
EVALUATOR_EMBEDDING = "text-embedding-ada-002"

# For different chunk
CHUNK_SIZE = 512
CHUNK_OVERLAP = 20


# Set the LLM, Embedding, and Text Splitter
Settings.llm = OpenAI(model=RAG_LLM, temperature=TEMPERATURE)
Settings.embed_model = OpenAIEmbedding(model=RAG_EMBEDDING)
Settings.text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)

"""# Test on different LLM model

### Evaluator LLM gpt-3.5-turbo/ RAG LLM gpt-3.5-turbo
"""

from datasets import load_dataset


# load a specific split of a subset dataset
ragbench_hotpotqa = load_dataset("rungalileo/ragbench", "hotpotqa", split="test")

ragbench_hotpotqa

print(ragbench_hotpotqa[0])

import pandas as pd
import json
import os

def save_hotpotqa_to_csv(dataset, output_dir='data/hotpotqa'):
    """
    Save an already loaded HotpotQA dataset to CSV file.

    Args:
        dataset: The loaded HotpotQA dataset
        output_dir (str): Directory path where the CSV file will be saved
    """
    # Create the output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Convert the dataset to a list of dictionaries
    data_list = []

    # Handle both single split and multiple split datasets
    if hasattr(dataset, 'keys'):
        # Multiple splits (train/test/validation)
        for split in dataset.keys():
            for item in dataset[split]:
                item_dict = item.copy()
                item_dict['split'] = split
                data_list.append(item_dict)
    else:
        # Single split
        for item in dataset:
            data_list.append(item)

    # Convert to DataFrame
    df = pd.DataFrame(data_list)

    # Convert nested structures to JSON strings for CSV compatibility
    for column in df.columns:
        if isinstance(df[column].iloc[0], (list, dict)):
            df[column] = df[column].apply(json.dumps)

    # Save to CSV
    output_path = os.path.join(output_dir, 'hotpotqa_dataset.csv')
    df.to_csv(output_path, index=False)
    print(f"Dataset saved to: {output_path}")

    # Print some basic statistics
    print("\nDataset Statistics:")
    print(f"Total number of samples: {len(df)}")
    if 'split' in df.columns:
        print("\nSamples per split:")
        print(df['split'].value_counts())

save_hotpotqa_to_csv(ragbench_hotpotqa)

# Load the documents
documents = SimpleDirectoryReader("data/hotpotqa/").load_data()
# Create the vector index
vector_index = VectorStoreIndex.from_documents(documents)
# Create the query engine
query_engine = vector_index.as_query_engine()

# For the evaluation
# Set the LLMs
generator_llm = OpenAI(model=GENERATOR_LLM)
critic_llm = OpenAI(model=CRITIC_LLM)
evaluator_llm = OpenAI(model=EVALUATOR_LLM)
# Set the embeddings
embeddings = OpenAIEmbedding(model=EVALUATOR_EMBEDDING)

print(documents[0])

# Define the evaluation metrics
metrics = [
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
]

# Create the testset generator
generator = TestsetGenerator.from_llama_index(
    generator_llm=generator_llm,
    critic_llm=critic_llm,
    embeddings=embeddings,
)

# Generate the testset
testset = generator.generate_with_llamaindex_docs(
    documents,
    test_size=10,
    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},
)

# Generate the testset
testset = generator.generate_with_llamaindex_docs(
    documents,
    test_size=10,
    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},
)


# convert to HF dataset
ds = testset.to_dataset()

print(ds[0])

result = evaluate(
    query_engine=query_engine,
    metrics=metrics,
    dataset=ds,
    llm=OpenAI(model=EVALUATOR_LLM),
    embeddings=OpenAIEmbedding(model=EVALUATOR_EMBEDDING),
)

# result_manual_df

# Convert the result to pandas
result_df = result.to_pandas()

result_df

import pandas as pd
# Calculating the averacge score for each metric in heatmap_data1
avg_faithfulness_1 = result_df['faithfulness'].mean()
avg_answer_relevancy_1 = result_df['answer_relevancy'].mean()
avg_context_precision_1 = result_df['context_precision'].mean()
avg_context_recall_1 = result_df['context_recall'].mean()
# Average scores for result_4o_df
avg_scores_1 = {
    'Metric': ['Faithfulness', 'Answer Relevancy', 'Context Precision', 'Context Recall'],
    'GPT-4o-turbo': [avg_faithfulness_1, avg_answer_relevancy_1, avg_context_precision_1, avg_context_recall_1]
}

# Combine into a DataFrame
comparison_table = pd.DataFrame(avg_scores_1)
print(comparison_table)