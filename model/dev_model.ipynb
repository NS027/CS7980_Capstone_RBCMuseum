{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to immprove the RAG System in Llamaindex\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This model contains four part\n",
    "- Data Ingestion\n",
    "- Data indexing\n",
    "- Storing\n",
    "- Query Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parsing & Ingestion\n",
    "- Data -> Data parsing + Ingestion -> Index\n",
    "\n",
    "### Data Querying\n",
    "- Index -> Retrival -> LLM + Prompts -> Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG - 5 Line Starter with LlamaIndex\n",
    "\n",
    "Naive RAG Failure modes at \n",
    "\n",
    "**Summarization**,**Comparison**, **Implicit Data** and **Multi-part Question**.\n",
    "\n",
    "Here's a simple 5-line starter code for LlamaIndex:\n",
    "\n",
    "1. **Load and Parse Documents**  \n",
    "   Use `SimpleDirectoryReader` to load documents from the directory:\n",
    "\n",
    "    ```\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    ```\n",
    "\n",
    "2. **Embed and Index the Documents**  \n",
    "   Create a vector index using the `from_documents` method:\n",
    "\n",
    "    ```\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    ```\n",
    "\n",
    "3. **Generate the Query Engine**  \n",
    "   Convert the index into a queryable engine:\n",
    "\n",
    "    ```\n",
    "    query_engine = index.as_query_engine()\n",
    "    ```\n",
    "\n",
    "4. **Run Queries**  \n",
    "   Use the query engine to ask questions:\n",
    "\n",
    "    ```\n",
    "    response = query_engine.query(\"What did the author do growing up?\")\n",
    "    ```\n",
    "\n",
    "5. **Print the Response**  \n",
    "   Output the response:\n",
    "\n",
    "    ```\n",
    "    print(response)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization\n",
    "\n",
    "**Ingestion**\n",
    "- Ingestion Pipeline\n",
    "    - transformation (sentenceSplitter(chunck_size, chunck_overlap))<br>\n",
    "    ```python\n",
    "    pipeline = IngestionPipeline(transformation[...])\n",
    "    nodes = pipeline.run(documents)\n",
    "    index = VectorStoreIndex(nodes)\n",
    "    ```\n",
    "    - \n",
    "    \n",
    "**Retriever**\n",
    "- See promts\n",
    "    - `promts_dic = query_engine.get_prompts()`\n",
    "    - 1. **Define the Prompt Template**  \n",
    "    Create a prompt string that instructs the model to answer in Shakespearean style:<br>\n",
    "        ```python\n",
    "        qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\\\n\"\n",
    "            \"--------------------\\\\n\"\n",
    "            \"{context_str}\\\\n\"\n",
    "            \"--------------------\\\\n\"\n",
    "            \"Given the context information and not prior knowledge, \"\n",
    "            \"answer the query in the style of a Shakespeare play.\\\\n\"\n",
    "            \"Query: {query_str}\\\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "        ```\n",
    "\n",
    "    - 2. **Create the Prompt Template Object**  \n",
    "    Wrap the string into a `PromptTemplate` object:\n",
    "\n",
    "        ```python\n",
    "        qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "        ```\n",
    "\n",
    "    - 3. **Update the Query Engine with Custom Prompts**  \n",
    "    Customize the query engine to use this template for responses:\n",
    "\n",
    "        ```python\n",
    "        query_engine.update_prompts(\n",
    "            {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    "        )\n",
    "        ```\n",
    "\n",
    "- Configuration retirever\n",
    "    - VectorIndexRetriever (similarity_top_k...) \n",
    "    - response_synthesizer = get_response_synthesizer(response_mode='tree_summarize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways to improve RAG\n",
    "\n",
    "1. **Data Quality**\n",
    "- LlamaParse\n",
    "- LlamaHub.ai\n",
    "\n",
    "\n",
    "2. **Query Sophistication**\n",
    "- Agentic RAG\n",
    "    - Multi-turn\n",
    "    - Query/task planning layer\n",
    "    - Tool interface for external environment\n",
    "    - Reflection\n",
    "    - Memory for personalization\n",
    "\n",
    "- 1. Routing\n",
    "    - Semantic Search + Summarization\n",
    "    - Router\n",
    "        - Vector Query Engine (retrieve top_k)\n",
    "        - Summary Query Engine (retrieve all)\n",
    "- 2. Conversation memory\n",
    "    - `chat_engine = index.as_chat_engine()`<br>`response = query_engine.chat(\"Query Quetiosn\")`\n",
    "- 3. Query planning\n",
    "    - `query_engine = SubQuestionQueryEngine`\n",
    "- 4. Tool use\n",
    "    - ReAct Agent\n",
    "\n",
    "- 3 Agent reasoning loops\n",
    "    - Sequential\n",
    "        - ReAct (Reason + Act)\n",
    "    - DAG-based\n",
    "        - Flow chart (Self-reflection)\n",
    "    - Tree-based\n",
    "        - come up with diffrent solution in experience\n",
    "\n",
    "- Query Strategies\n",
    "    - SubQuestionQueryEngine\n",
    "    - Small-to-Big retrieval\n",
    "    - Metadata filtering\n",
    "    - Auto-retrieval\n",
    "    - Hybrid search\n",
    "\n",
    "- Retriever\n",
    "    - RecursiveRetriever\n",
    "    - SQLTableRetrieverQueryEngine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-document agents\n",
    "\n",
    "- Create seperate query engines\n",
    "```python\n",
    "documents = SimpleDirectoryReader (\"2020\").load_data ( )\n",
    "index2020 = VectorStoreIndex. from_documents (documents)\n",
    "query_engine_2020 = index2020.as._query_engine( )\n",
    "\n",
    "documents = SimpleDirectoryReader (\"2021\").load_data()\n",
    "index2021 = VectorStoreIndex. from_documents (documents)\n",
    "query_engine_2021 = index2021.as_query_engine()\n",
    "\n",
    "documents = SimpleDirectoryReader (\"2022\" ).load_data ()\n",
    "index2022 = VectorStoreIndex. from_documents (documents)\n",
    "query_engine_2022 =index2022.as_query_engine()\n",
    "```\n",
    "- Define tools<br>\n",
    "**Define the Query Engine Tools**  \n",
    "   You can configure a tool with relevant metadata and description:\n",
    "\n",
    "    ```python\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=query_engine_2020,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"2020_facts_tool\",\n",
    "                description=(\n",
    "                    \"Contains facts about filings \"\n",
    "                    \"about the company from the year 2020\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        # ... etc ...\n",
    "    ]\n",
    "    ```\n",
    "- Define agent\n",
    "```python\n",
    "function_llm = OpenAI(model='gpt-4o')\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=function_llm,\n",
    "    system_prompt=f\"\"\"\\\n",
    "    You are a specialized agent designed to answer queries about financial data.\n",
    "    You must ALWAYS use at least one of the tools provided when answering any query.\n",
    "        \"\"\",\n",
    "\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
