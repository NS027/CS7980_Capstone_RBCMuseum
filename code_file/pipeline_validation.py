# -*- coding: utf-8 -*-
"""combine_test copy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EFX-61n9dPDTBRm3iPjGvdFqYTYuoaxD
"""

import os
from llama_index.core import SimpleDirectoryReader
from llama_index.core import VectorStoreIndex
from llama_index.core.node_parser import TokenTextSplitter
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings
from ragas.testset.generator import TestsetGenerator
from ragas.testset.evolutions import simple, reasoning, multi_context
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from ragas.integrations.llama_index import evaluate



# Get the API key from the environment
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")


# For different LLM models
RAG_LLM = "gpt-3.5-turbo"
GENERATOR_LLM = "gpt-3.5-turbo"
CRITIC_LLM = "gpt-3.5-turbo"
EVALUATOR_LLM = "gpt-3.5-turbo"

# For different temperatures
TEMPERATURE = 0.1

# For different embeddings
RAG_EMBEDDING = "text-embedding-ada-002"
EVALUATOR_EMBEDDING = "text-embedding-ada-002"

# For different chunk
CHUNK_SIZE = 512
CHUNK_OVERLAP = 20


# Set the LLM, Embedding, and Text Splitter
Settings.llm = OpenAI(model=RAG_LLM, temperature=TEMPERATURE)
Settings.embed_model = OpenAIEmbedding(model=RAG_EMBEDDING)
Settings.text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)

"""# Test on different LLM model

### Evaluator LLM gpt-3.5-turbo/ RAG LLM gpt-3.5-turbo
"""

from datasets import load_dataset


# load a specific split of a subset dataset
ragbench_hotpotqa = load_dataset("rungalileo/ragbench", "hotpotqa", split="test")

ragbench_hotpotqa

print(ragbench_hotpotqa[0])

import pandas as pd
import json
import os

def save_hotpotqa_to_csv(dataset, output_dir='data/hotpotqa'):
    """
    Save an already loaded HotpotQA dataset to CSV file.

    Args:
        dataset: The loaded HotpotQA dataset
        output_dir (str): Directory path where the CSV file will be saved
    """
    # Create the output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Convert the dataset to a list of dictionaries
    data_list = []

    # Handle both single split and multiple split datasets
    if hasattr(dataset, 'keys'):
        # Multiple splits (train/test/validation)
        for split in dataset.keys():
            for item in dataset[split]:
                item_dict = item.copy()
                item_dict['split'] = split
                data_list.append(item_dict)
    else:
        # Single split
        for item in dataset:
            data_list.append(item)

    # Convert to DataFrame
    df = pd.DataFrame(data_list)

    # Convert nested structures to JSON strings for CSV compatibility
    for column in df.columns:
        if isinstance(df[column].iloc[0], (list, dict)):
            df[column] = df[column].apply(json.dumps)

    # Save to CSV
    output_path = os.path.join(output_dir, 'hotpotqa_dataset.csv')
    df.to_csv(output_path, index=False)
    print(f"Dataset saved to: {output_path}")

    # Print some basic statistics
    print("\nDataset Statistics:")
    print(f"Total number of samples: {len(df)}")
    if 'split' in df.columns:
        print("\nSamples per split:")
        print(df['split'].value_counts())

save_hotpotqa_to_csv(ragbench_hotpotqa)

# Load the documents
documents = SimpleDirectoryReader("data/hotpotqa/").load_data()
# Create the vector index
vector_index = VectorStoreIndex.from_documents(documents)
# Create the query engine
query_engine = vector_index.as_query_engine()

# For the evaluation
# Set the LLMs
generator_llm = OpenAI(model=GENERATOR_LLM)
critic_llm = OpenAI(model=CRITIC_LLM)
evaluator_llm = OpenAI(model=EVALUATOR_LLM)
# Set the embeddings
embeddings = OpenAIEmbedding(model=EVALUATOR_EMBEDDING)

print(documents[0])

# Define the evaluation metrics
metrics = [
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
]

# Create the testset generator
generator = TestsetGenerator.from_llama_index(
    generator_llm=generator_llm,
    critic_llm=critic_llm,
    embeddings=embeddings,
)

# Generate the testset
testset = generator.generate_with_llamaindex_docs(
    documents,
    test_size=10,
    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},
)

# Generate the testset
testset = generator.generate_with_llamaindex_docs(
    documents,
    test_size=20,
    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},
)


# convert to HF dataset
ds = testset.to_dataset()

from datasets import load_dataset, Dataset
import pandas as pd

# Load the Hugging Face dataset
ragbench_hotpotqa = load_dataset("rungalileo/ragbench", "hotpotqa", split="test")



# Assume we got: dict_keys(['question', 'documents', 'answer', 'evolution_type'])

# Create the manually generated dataset based on verified keys
def create_manually_generated_ds(ragbench_dataset):
    manually_generated_data = []

    for item in ragbench_dataset:
        # Creating an entry that matches the simplified structure of `ds`
        manually_generated_entry = {
            'question': item.get('question', ''),
            'contexts': item.get('documents_sentences', []),
            'ground_truth': item.get('response', ''),
            'evolution_type': item.get('evolution_type', 'simple')
        }
        manually_generated_data.append(manually_generated_entry)

    return manually_generated_data

# Create manually generated dataset
manually_generated_ds = create_manually_generated_ds(ragbench_hotpotqa)

# Convert to a DataFrame to verify structure
manually_generated_ds_df = pd.DataFrame(manually_generated_ds)
manually_generated_ds_hf = Dataset.from_pandas(manually_generated_ds_df)


# Printing the first entry to verify the structure matches `ds`
print(manually_generated_ds[0])

print(ds[0])

result = evaluate(
    query_engine=query_engine,
    metrics=metrics,
    dataset=ds,
    llm=OpenAI(model=EVALUATOR_LLM),
    embeddings=OpenAIEmbedding(model=EVALUATOR_EMBEDDING),
)

manually_generated_ds_hf_sample = manually_generated_ds_hf.select(range(20))
result_manual = evaluate(
    query_engine=query_engine,
    metrics=metrics,
    dataset=manually_generated_ds_hf_sample,
    llm=OpenAI(model=EVALUATOR_LLM),
    embeddings=OpenAIEmbedding(model=EVALUATOR_EMBEDDING),
)
result_manual_df = result_manual.to_pandas()

result_manual_df

# Convert the result to pandas
result_df = result.to_pandas()

result_df

# Calculating the averacge score for each metric in heatmap_data1
avg_faithfulness_1 = result_df['faithfulness'].mean()
avg_answer_relevancy_1 = result_df['answer_relevancy'].mean()
avg_context_precision_1 = result_df['context_precision'].mean()
avg_context_recall_1 = result_df['context_recall'].mean()

heatmap_data1 = pd.DataFrame({
    "Faithfulness": [avg_faithfulness_1],
    "Answer Relevancy": [avg_answer_relevancy_1],
    "Context Precision": [avg_context_precision_1],
    "Context Recall": [avg_context_recall_1]
}, index=["Average"])

# Calculating the average score for each metric in heatmap_data2
result_manual_df = result_manual_df.dropna(subset=['faithfulness'])
avg_faithfulness_2 = result_manual_df['faithfulness'].mean()
avg_answer_relevancy_2 = result_manual_df['answer_relevancy'].mean()
avg_context_precision_2 = result_manual_df['context_precision'].mean()
avg_context_recall_2 = result_manual_df['context_recall'].mean()

heatmap_data2 = pd.DataFrame({
    "Faithfulness": [avg_faithfulness_2],
    "Answer Relevancy": [avg_answer_relevancy_2],
    "Context Precision": [avg_context_precision_2],
    "Context Recall": [avg_context_recall_2]
}, index=["Average"])

# Plotting the heatmaps side by side
fig, axes = plt.subplots(2,1, figsize=(12, 6))

# Heatmap 1
sns.heatmap(heatmap_data1, cmap="crest",annot=True, cbar=True, square=True, fmt=".3f", ax=axes[0])
axes[0].set_title("LLM generated dataset")
axes[0].tick_params(axis='x', rotation=0)
axes[0].tick_params(axis='y', rotation=0)

# Heatmap 2
sns.heatmap(heatmap_data2, cmap="crest",annot=True, cbar=True, square=True, fmt=".3f", ax=axes[1])
axes[1].set_title("Manual parsed dataset")
axes[1].tick_params(axis='x', rotation=0)
axes[1].tick_params(axis='y', rotation=0)

plt.tight_layout()
plt.show()

csv_5 = result_df.to_csv("result_5.csv")

ds_5 = ds.to_csv("ds_5.csv")