{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is multi files agent\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"data/three/metagpt.pdf\",\n",
    "    \"data/three/longlora.pdf\",\n",
    "    \"data/three/selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Chunk and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"data/three\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Tools\n",
    "- vector tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "def vector_query(\n",
    "    query : str,\n",
    "    page_numbers: List[str],\n",
    ") -> str:\n",
    "    \"\"\"Performs a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to embeded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to search\n",
    "    over all pages. Otherwise, filter by the set of sepcified pages.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p}\n",
    "        for p in page_numbers\n",
    "    ]\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_query\",\n",
    "    fn=vector_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summary tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode='tree_summarize',\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        'Useful if you want to get a summary of the ragchecker.'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search by the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: data/three/metagpt.pdf\n",
      "Getting tools for paper: data/three/longlora.pdf\n",
      "Getting tools for paper: data/three/selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    paper_to_tools_dict[paper] = [vector_query_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put into a flat list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Model Setting\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each paper has TWO tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall angent worker\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with quetsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me the evaluation results.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_query with args: {\"query\": \"evaluation dataset used in LongLoRA\", \"page_numbers\": [\"7\"]}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in LongLoRA is the book corpus dataset PG19 and the cleaned Arxiv Math proof-pile dataset.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_query with args: {\"query\": \"evaluation results of LongLoRA\", \"page_numbers\": [\"7\"]}\n",
      "=== Function Output ===\n",
      "LongLoRA achieves promising results on extremely large settings, with some perplexity degradation observed on small context sizes for the extended models. It is noted that LongLoRA performs well in retrieval tasks on long contexts, showing comparable performance to state-of-the-art models with lower fine-tuning costs.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA consists of the book corpus dataset PG19 and the cleaned Arxiv Math proof-pile dataset. \n",
      "\n",
      "Regarding the evaluation results of LongLoRA, it achieves promising results on extremely large settings. Some perplexity degradation is observed on small context sizes for the extended models. LongLoRA performs well in retrieval tasks on long contexts, showing comparable performance to state-of-the-art models with lower fine-tuning costs.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA,\"\n",
    "    \" and then tell me the evaluation results.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Gvie me a summary of both Self-RAG and LongLoRA.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models through retrieval on demand and self-reflection. It trains a language model to learn to retrieve, generate, and critique text passages and its own generation using special tokens called reflection tokens. This method allows for tailored behaviors at test time by leveraging reflection tokens, leading to significant improvements in performance, factuality, and citation accuracy compared to other models.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context sizes of pre-trained large language models with limited computation cost. It combines improved LoRA with shifted sparse attention to achieve strong empirical results on various tasks with Llama2 models. LongLoRA extends the context window of LLMs while retaining their original architectures and is compatible with most existing techniques like Flash-Attention2. Additionally, LongLoRA is an innovative meta-programming framework that incorporates efficient human workflows into Large Language Model-based multi-agent collaborations, encoding Standardized Operating Procedures into prompt sequences for streamlined workflows.\n",
      "=== LLM Response ===\n",
      "Here are the summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models through retrieval on demand and self-reflection. It trains a language model to learn to retrieve, generate, and critique text passages and its own generation using special tokens called reflection tokens. This method allows for tailored behaviors at test time by leveraging reflection tokens, leading to significant improvements in performance, factuality, and citation accuracy compared to other models.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context sizes of pre-trained large language models with limited computation cost. It combines improved LoRA with shifted sparse attention to achieve strong empirical results on various tasks with Llama2 models. LongLoRA extends the context window of LLMs while retaining their original architectures and is compatible with most existing techniques like Flash-Attention2. Additionally, LongLoRA is an innovative meta-programming framework that incorporates efficient human workflows into Large Language Model-based multi-agent collaborations, encoding Standardized Operating Procedures into prompt sequences for streamlined workflows.\n",
      "Here are the summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models through retrieval on demand and self-reflection. It trains a language model to learn to retrieve, generate, and critique text passages and its own generation using special tokens called reflection tokens. This method allows for tailored behaviors at test time by leveraging reflection tokens, leading to significant improvements in performance, factuality, and citation accuracy compared to other models.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context sizes of pre-trained large language models with limited computation cost. It combines improved LoRA with shifted sparse attention to achieve strong empirical results on various tasks with Llama2 models. LongLoRA extends the context window of LLMs while retaining their original architectures and is compatible with most existing techniques like Flash-Attention2. Additionally, LongLoRA is an innovative meta-programming framework that incorporates efficient human workflows into Large Language Model-based multi-agent collaborations, encoding Standardized Operating Procedures into prompt sequences for streamlined workflows.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Gvie me a summary of both Self-RAG and LongLoRA.\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrival tools for multiple docs\n",
    "[Watch it here](https://www.bilibili.com/video/BV14t421u7AM/?spm_id_from=333.788&vd_source=674819236ef139055aebeaab34e08502)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
