{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a agent reasoning model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setting\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Chunk and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(input_files=[\"data/ragchecker.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "def vector_query(\n",
    "    query : str,\n",
    "    page_numbers: List[str],\n",
    ") -> str:\n",
    "    \"\"\"Performs a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to embeded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to search\n",
    "    over all pages. Otherwise, filter by the set of sepcified pages.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p}\n",
    "        for p in page_numbers\n",
    "    ]\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_query\",\n",
    "    fn=vector_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode='tree_summarize',\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        'Useful if you want to get a summary of the ragchecker.'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Model Setting\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent of llamaindex\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[vector_query_tool, summary_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test for result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- response from gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the ragchecker metric, and how to evaluate the performance.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"ragchecker\"}\n",
      "=== Function Output ===\n",
      "RAGCHECKER is an evaluation framework specifically designed for Retrieval-Augmented Generation (RAG) systems. It offers a comprehensive suite of diagnostic metrics to assess the performance of both the retrieval and generation modules within RAG systems. The framework has been validated through human assessments, demonstrating a strong correlation with human evaluations. By evaluating various RAG systems across diverse domain datasets, RAGCHECKER provides valuable insights into the behaviors of the retriever and generator components, highlighting trade-offs in RAG system designs and offering guidance for future advancements in RAG applications.\n",
      "=== LLM Response ===\n",
      "The RAGCHECKER metric is an evaluation framework tailored for Retrieval-Augmented Generation (RAG) systems. It provides a range of diagnostic metrics to assess the performance of both the retrieval and generation modules within RAG systems. The framework has been validated through human assessments and shows a strong correlation with human evaluations.\n",
      "\n",
      "To evaluate the performance of RAG systems using the RAGCHECKER metric, you can analyze the metrics provided by the framework to understand the behaviors of the retriever and generator components. By evaluating different RAG systems across various domain datasets, RAGCHECKER can help identify trade-offs in RAG system designs and offer insights for enhancing RAG applications.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the ragchecker metric,\"\n",
    "    \" and how to evaluate the performance.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: ragchecker.pdf\n",
      "file_path: data/ragchecker.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 2553412\n",
      "creation_date: 2024-09-30\n",
      "last_modified_date: 2024-09-30\n",
      "\n",
      "RAGCHECKER : A Fine-grained Framework for\n",
      "Diagnosing Retrieval-Augmented Generation\n",
      "Dongyu Ru1∗Lin Qiu1∗Xiangkun Hu1∗Tianhang Zhang1∗Peng Shi1∗\n",
      "Shuaichen Chang1∗Cheng Jiayang1†Cunxiang Wang1†Shichao Sun2\n",
      "Huanyu Li2Zizhao Zhang1†Binjie Wang1†Jiarong Jiang1Tong He1\n",
      "Zhiguo Wang1Pengfei Liu2Yue Zhang3Zheng Zhang1\n",
      "1Amazon AWS AI2Shanghai Jiaotong University3Westlake University\n",
      "Abstract\n",
      "Despite Retrieval-Augmented Generation (RAG) showing promising capability in\n",
      "leveraging external knowledge, a comprehensive evaluation of RAG systems is still\n",
      "challenging due to the modular nature of RAG, evaluation of long-form responses\n",
      "and reliability of measurements. In this paper, we propose a fine-grained evaluation\n",
      "framework, RAGCHECKER , that incorporates a suite of diagnostic metrics for both\n",
      "the retrieval and generation modules. Meta evaluation verifies that RAGCHECKER\n",
      "has significantly better correlations with human judgments than other evaluation\n",
      "metrics. Using RAGCHECKER , we evaluate 8 RAG systems and conduct an in-\n",
      "depth analysis of their performance, revealing insightful patterns and trade-offs in\n",
      "the design choices of RAG architectures. The metrics of RAGCHECKER can guide\n",
      "researchers and practitioners in developing more effective RAG systems3.\n",
      "1 Introduction\n",
      "Retrieval-Augmented Generation (RAG) systems [ 18,7] enhance Large Language Models (LLMs) by\n",
      "incorporating external knowledge bases, enabling more precise and contextually relevant responses [ 7,\n",
      "53,13]. As these systems become integral to a variety of applications [ 54,2,8], it’s imperative to\n",
      "develop robust and comprehensive evaluation frameworks to assess their performance and identify\n",
      "areas for improvement. Evaluating RAG systems, however, presents several challenges:\n",
      "(1)modular complexity : The modular nature of RAG systems, comprising both a retriever and a\n",
      "generator, complicates the design of effective evaluation metrics. It is crucial to establish metrics\n",
      "that can holistically assess the entire system as well as evaluate the individual modules and their\n",
      "interplay [ 53], allowing for fully understanding the sources of the errors and misses and how they\n",
      "are generated. (2) metric limitation : Existing metrics for evaluating RAG systems, which are often\n",
      "rule-based or coarse-grained, fall short in providing accurate and interpretable results. Specifically,\n",
      "traditional metrics like recall@k and MRR [ 44] for retrievers depend on annotated chunks and a rigid\n",
      "chunking approach, missing out on the full semantic scope of the knowledge base. For generators,\n",
      "typical measures such as n-gram-based (e.g., BLEU [ 30], ROUGE [ 19]), embedding-based (e.g.,\n",
      "BERTScore [ 56]), and LLM-based methods [ 45] perform well with concise answers but fail to\n",
      "detect finer distinctions in longer responses. To bridge these gaps, it is essential to develop detailed,\n",
      "semantic-based evaluation metrics that effectively capture the intricacies and overall quality of both\n",
      "the retrieval and generation components in RAG systems. (3) metric reliability : the reliability\n",
      "∗Shared first authorship.\n",
      "†Work done during internship at Amazon.\n",
      "3This work has been open sourced at https://github.com/amazon-science/RAGChecker\n",
      "Preprint. Under review.arXiv:2408.08067v2  [cs.CL]  17 Aug 2024\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- result under gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the ragchecker metric, and how to evaluate the performance.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"ragchecker metric\"}\n",
      "=== Function Output ===\n",
      "The RAGCHECKER framework introduces a suite of metrics for evaluating Retrieval-Augmented Generation (RAG) systems. These metrics cover aspects such as claim recall, context precision, context utilization, noise sensitivity, faithfulness, precision, recall, F1 score, hallucination, self-knowledge, Correct Retrieval (CR), Correct Prediction (CP), Correct Update (CU), Non-Stopword Overlap (NS), Semantic Knowledge (SK), and more.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_query with args: {\"query\": \"evaluate the performance of ragchecker metric\", \"page_numbers\": []}\n",
      "=== Function Output ===\n",
      "RAGCHECKER metric's performance was evaluated by comparing its correlations with human judgments to other evaluation metrics. The meta-evaluation confirmed that RAGCHECKER had significantly better correlations with human judgments, indicating its effectiveness in capturing the quality and reliability of RAG systems from a human perspective.\n",
      "=== LLM Response ===\n",
      "The **RAGCHECKER** framework introduces a comprehensive suite of metrics designed to evaluate Retrieval-Augmented Generation (RAG) systems. These metrics include:\n",
      "\n",
      "- **Claim Recall**: Measures the ability to recall relevant claims.\n",
      "- **Context Precision**: Assesses the precision of the context used.\n",
      "- **Context Utilization**: Evaluates how effectively the context is utilized.\n",
      "- **Noise Sensitivity**: Determines the system's sensitivity to noise.\n",
      "- **Faithfulness**: Checks the faithfulness of the generated content to the source.\n",
      "- **Precision, Recall, F1 Score**: Standard metrics for evaluating the accuracy and completeness of the system.\n",
      "- **Hallucination**: Measures the extent of hallucinated content.\n",
      "- **Self-Knowledge**: Evaluates the system's self-awareness.\n",
      "- **Correct Retrieval (CR)**: Assesses the accuracy of the retrieval process.\n",
      "- **Correct Prediction (CP)**: Measures the correctness of the predictions made.\n",
      "- **Correct Update (CU)**: Evaluates the accuracy of updates made.\n",
      "- **Non-Stopword Overlap (NS)**: Measures the overlap of non-stopwords between the generated content and the source.\n",
      "- **Semantic Knowledge (SK)**: Assesses the semantic understanding of the content.\n",
      "\n",
      "### Evaluating Performance\n",
      "\n",
      "The performance of the RAGCHECKER metric was evaluated by comparing its correlations with human judgments to other evaluation metrics. The meta-evaluation confirmed that RAGCHECKER had significantly better correlations with human judgments. This indicates its effectiveness in capturing the quality and reliability of RAG systems from a human perspective.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the ragchecker metric,\"\n",
    "    \" and how to evaluate the performance.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent reasoning loop\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation daatsets used.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"evaluation datasets\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets used in the RAGCHECKER framework are repurposed from existing open-domain question answering datasets, including RobustQA, KIWI, ClapNQ, and NovelQA. The datasets cover various domains such as Biomedical, Finance, Lifestyle, Recreation, Technology, Science, and Writing. The short answers in these datasets are converted to long-form answers to match the capabilities of modern RAG systems. Additionally, the datasets are curated to ensure that the long-form answers are generated accurately without any hallucinations. The benchmark includes a total of 4,162 questions across these domains.\n",
      "=== LLM Response ===\n",
      "The evaluation datasets used in the RAGCHECKER framework are repurposed from existing open-domain question answering datasets, including RobustQA, KIWI, ClapNQ, and NovelQA. The datasets cover various domains such as Biomedical, Finance, Lifestyle, Recreation, Technology, Science, and Writing. The short answers in these datasets are converted to long-form answers to match the capabilities of modern RAG systems. Additionally, the datasets are curated to ensure that the long-form answers are generated accurately without any hallucinations. The benchmark includes a total of 4,162 questions across these domains.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me about the evaluation daatsets used.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the results over one of the above  datasets.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_query with args: {\"query\": \"evaluation results\", \"page_numbers\": [\"1\"]}\n",
      "=== Function Output ===\n",
      "The evaluation results of the RAG systems were obtained using a fine-grained evaluation framework called RAGCHECKER. This framework incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation has shown that RAGCHECKER has significantly better correlations with human judgments compared to other evaluation metrics. Through the use of RAGCHECKER, the performance of 8 RAG systems was evaluated, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics provided by RAGCHECKER can guide researchers and practitioners in developing more effective RAG systems.\n",
      "=== LLM Response ===\n",
      "The evaluation results of the RAG systems over the specified dataset were obtained using the RAGCHECKER framework. The framework includes a comprehensive set of diagnostic metrics for evaluating both the retrieval and generation modules of RAG systems. Meta-evaluation has demonstrated that RAGCHECKER exhibits strong correlations with human judgments, outperforming other evaluation metrics.\n",
      "\n",
      "The evaluation involved assessing the performance of 8 RAG systems, revealing valuable insights into the design choices and trade-offs in RAG architectures. The metrics provided by RAGCHECKER can serve as a guide for researchers and practitioners in enhancing the effectiveness of RAG systems.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me the results over one of the above  datasets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a task\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[vector_query_tool, summary_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "     \"Tell me about the ragchecker metric,\"\n",
    "    \" and how to evaluate the performance of RAG.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the ragchecker metric, and how to evaluate the performance of RAG.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"ragchecker\"}\n",
      "=== Function Output ===\n",
      "RAGCHECKER is an evaluation framework specifically designed for Retrieval-Augmented Generation (RAG) systems. It assesses both the retrieval and generation components of RAG systems using various diagnostic metrics such as precision, recall, faithfulness, context precision, context utilization, noise sensitivity, hallucination, and self-knowledge. The framework has been validated through human assessments and has shown a strong correlation with human evaluations. It has been used to evaluate 8 different RAG systems across 10 diverse domain datasets, providing valuable insights into the behaviors of the retriever and generator components, highlighting trade-offs in RAG system designs, and offering guidance for future advancements in RAG applications.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how many steps completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num completed for task 4c08a022-0073-4a48-ba94-ffd1ad4d171f: 1\n",
      "RAGCHECKER is an evaluation framework specifically designed for Retrieval-Augmented Generation (RAG) systems. It assesses both the retrieval and generation components of RAG systems using various diagnostic metrics such as precision, recall, faithfulness, context precision, context utilization, noise sensitivity, hallucination, and self-knowledge. The framework has been validated through human assessments and has shown a strong correlation with human evaluations. It has been used to evaluate 8 different RAG systems across 10 diverse domain datasets, providing valuable insights into the behaviors of the retriever and generator components, highlighting trade-offs in RAG system designs, and offering guidance for future advancements in RAG applications.\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
    "print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num completed for task 4c08a022-0073-4a48-ba94-ffd1ad4d171f: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskStep(task_id='4c08a022-0073-4a48-ba94-ffd1ad4d171f', step_id='2aa255ba-ca01-49e7-951c-ea2e68e68b85', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Num completed for task {task.task_id}: {len(upcoming_steps)}\")\n",
    "upcoming_steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What are the parameter used for evaluation?\n",
      "=== LLM Response ===\n",
      "The parameters used for evaluating the performance of RAG systems using the RAGCHECKER metric include:\n",
      "\n",
      "1. Precision: Measures the proportion of generated responses that are correct and relevant to the input query.\n",
      "\n",
      "2. Recall: Measures the proportion of relevant responses that are generated by the system.\n",
      "\n",
      "3. Faithfulness: Evaluates the extent to which the generated responses are faithful to the retrieved context.\n",
      "\n",
      "4. Context Precision: Measures the proportion of generated responses that are contextually relevant to the retrieved context.\n",
      "\n",
      "5. Context Utilization: Assesses how effectively the system utilizes the retrieved context to generate responses.\n",
      "\n",
      "6. Noise Sensitivity: Measures the system's sensitivity to noise in the retrieved context.\n",
      "\n",
      "7. Hallucination: Evaluates the extent to which the system generates responses that are not supported by the retrieved context.\n",
      "\n",
      "8. Self-Knowledge: Assesses the system's ability to recognize its own limitations and provide appropriate responses.\n",
      "\n",
      "These parameters provide a comprehensive evaluation of the retrieval and generation components of RAG systems, helping to identify strengths, weaknesses, and areas for improvement in the system's performance.\n"
     ]
    }
   ],
   "source": [
    "# One more step\n",
    "step_output = agent.run_step(\n",
    "    task.task_id,\n",
    "    input=\"What are the parameter used for evaluation?\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final answer for last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#step_output = agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
