{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is tools calling model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre-setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulates the function tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two numbers together\"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def mystry(x: int, y: int) -> int:\n",
    "    \"\"\"Mystery function that operates on top of two numbers\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystry_tool = FunctionTool.from_defaults(fn=mystry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integrate it with the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystry with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\",)\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystry_tool], 'Tell me the output of the mystery function on 2 and 9',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourcing the specific page with medadata\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(input_files=[\"data/ragchecker.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take a look at the meta data:\n",
    "```\n",
    "page_label: 1\n",
    "file_name: ragchecker.pdf\n",
    "file_path: data/ragchecker.pdf\n",
    "file_type: application/pdf\n",
    "file_size: 2553412\n",
    "creation_date: 2024-09-30\n",
    "last_modified_date: 2024-09-30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 5\n",
      "file_name: ragchecker.pdf\n",
      "file_path: data/ragchecker.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 2553412\n",
      "creation_date: 2024-09-30\n",
      "last_modified_date: 2024-09-30\n",
      "\n",
      "3.3.1 Overall Metrics\n",
      "To assess the overall response quality of a RAG system from a user’s perspective, we can compute the\n",
      "precision and recall at claim level for each model generated response against its paired ground-truth\n",
      "answer. Specifically, we first extract claims from a model response mand a ground-truth answer gtas\n",
      "{c(m)\n",
      "i}and{c(gt)\n",
      "i}respectively. Then, we define correct claims in the response as {c(m)\n",
      "i|c(m)\n",
      "i∈gt},\n",
      "and correct claims in the ground-truth answer as {c(gt)\n",
      "i|c(gt)\n",
      "i∈m}. Two metrics can be computed\n",
      "directly: precision is the proportion of correct claims in all response claims, and recall is the\n",
      "proportion of correct claims in all ground-truth answer claims. Further, the harmonic average of\n",
      "precision and recall gives the F1score, as the overall performance metric.\n",
      "3.3.2 Retriever Metrics\n",
      "Ideally, a perfect retriever returns precisely all claims needed to generate the ground-truth answer.\n",
      "Completeness-wise, we can measure how many claims made in the ground-truth answer are covered\n",
      "by retrieved chunks. With retrieved chunks as the reference text, we compute claim recall as the\n",
      "proportion of {c(gt)\n",
      "i|c(gt)\n",
      "i∈ {chunk j}}.\n",
      "Differently, we define the retriever precision at chunk-level instead of claim-level. A retrieved chunk\n",
      "is called relevant chunk (r-chunk ), if any ground-truth claim is entailed in it. In other words, chunk j\n",
      "is a relevant chunk if ∃i, s.t. c(gt)\n",
      "i∈chunk j. The rest retrieved chunks are called irrelevant chunk\n",
      "(irr-chunk ). The retriever’s context precision is defined as |{r-chunk j}|/k, where kis the number of\n",
      "all retrieved chunks.\n",
      "Note that a chunk-level precision provides better interpretability than a claim-level one, because\n",
      "in practice RAG systems usually work with documents processed to be text chunks in a fixed size.\n",
      "That being said, it is likely that a chunk may contain relevant claims and irrelevant or misleading\n",
      "information at the same time. As a result, the best possible retriever can only achieve a claim-level\n",
      "precision score lower than 100%, and such an upper-bound varies depending on the actual text\n",
      "distribution in Dand chunking strategy.\n",
      "3.3.3 Generator Metrics\n",
      "Given kretrieved chunks (possibly mixing relevant and irrelevant information), a perfect generator\n",
      "would identify and include all ground-truth-relevant claims and ignore any that are not. Because the\n",
      "generator’s results have dependency on retrieved chunks, we provide in total six metrics characterizing\n",
      "different aspects of its performance.\n",
      "Given a model response mand its claims {c(m)\n",
      "i}, we first compute the proportion of c(m)\n",
      "ithat are\n",
      "entailed in retrieved chunks. This metric is faithfulness , as it describes how faithful the generator is\n",
      "to the provided context, thus the higher the better.\n",
      "Next, we examine three types of incorrect response claims, i.e. {c(m)\n",
      "i|c(m)\n",
      "i/∈gt}.\n",
      "1.The first type includes incorrect claim that are entailed in a relevant chunk, then it indicates\n",
      "the generator is sensitive to noise coupled with useful information. The proportion of this\n",
      "type of claims to all {c(m)\n",
      "i}isrelevant noise sensitivity .\n",
      "2.The second type includes incorrect claim that are entailed in an irrelevant chunk, then it\n",
      "indicates the generator is also sensitive to noise even in an irrelevant context. The proportion\n",
      "of these incorrect claims is irrelevant noise sensitivity .\n",
      "3.Finally, the third type includes incorrect claims that are not entailed in any retrieved chunk,\n",
      "meaning all such claims are generated by the generator itself. Its proportion is hallucination .\n",
      "Note that for simplicity we group the two noise sensitivities in Fig. 1, but later in Sec. 4.3 we can see\n",
      "that generators generally has different sensitivity to relevant and irrelevant noise.\n",
      "Finally, we characterize how a generator uses information sources to produce correct claims. A\n",
      "correct claim not entailed by any chunk can only be based on generator’s self-knowledge , thus\n",
      "the proportion of these claims reflects how many correct claims are generated on its own. A lower\n",
      "self-knowledge score is better, when the generator is expected to fully depend on retrieved context\n",
      "only in a RAG system. On the other hand, we also check how much retrieved relevant information is\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(nodes[4].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a vector index query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(sililarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using metadata to filter the rage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"5\"},\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"what is the ragchecker metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RAGchecker metrics include precision, recall, and F1 score at claim level for assessing overall response quality, claim recall for measuring completeness of retrieved chunks, retriever precision at chunk-level, faithfulness metric for generator performance, relevant noise sensitivity, irrelevant noise sensitivity, hallucination metric, and self-knowledge metric for characterizing how a generator produces correct claims based on information sources.\n",
      "\n",
      "=============================\n",
      "\n",
      "Final Response: The RAGchecker metrics include precision, recall, and\n",
      "F1 score at claim level for assessing overall response quality, claim\n",
      "recall for measuring completeness of retrieved chunks, retriever\n",
      "precision at chunk-level, faithfulness metric for generator\n",
      "performance, relevant noise sensitivity, irrelevant noise sensitivity,\n",
      "hallucination metric, and self-knowledge metric for characterizing how\n",
      "a generator produces correct claims based on information sources.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "print(response)\n",
    "print(\"\\n=============================\\n\")\n",
    "\n",
    "pprint_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '5', 'file_name': 'ragchecker.pdf', 'file_path': 'data/ragchecker.pdf', 'file_type': 'application/pdf', 'file_size': 2553412, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enchancing Data retrieval\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- perform a vector search over an index along with the page numbers as a matadata filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def vector_query(\n",
    "    query : str,\n",
    "    page_numbers: List[str],\n",
    ") -> str:\n",
    "    \"\"\"Performs a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to embeded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to search\n",
    "    over all pages. Otherwise, filter by the set of sepcified pages.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p}\n",
    "        for p in page_numbers\n",
    "    ]\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_query\",\n",
    "    fn=vector_query,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calling the llm with the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_query with args: {\"query\": \"ragchecker metrics\", \"page_numbers\": [\"5\"]}\n",
      "=== Function Output ===\n",
      "The metrics for evaluating a RAG system include precision, recall, and F1 score at claim level for overall response quality assessment. Additionally, retriever metrics involve measuring claim recall and retriever precision at chunk-level. Generator metrics include faithfulness, relevant noise sensitivity, irrelevant noise sensitivity, hallucination, and self-knowledge scores to assess the generator's performance in producing correct claims based on retrieved chunks.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool], \n",
    "    'What is the ragchecker metrics as described in the page 5',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '5', 'file_name': 'ragchecker.pdf', 'file_path': 'data/ragchecker.pdf', 'file_type': 'application/pdf', 'file_size': 2553412, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall tool system\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a summary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode='tree_summarize',\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        'Useful if you want to get a summary of the ragchecker.'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_query with args: {\"query\": \"averaged evaluation results for 8 RAG systems across 10 diverse domain datasets\", \"page_numbers\": [\"9\"]}\n",
      "=== Function Output ===\n",
      "The averaged evaluation results for the 8 RAG systems across 10 diverse domain datasets showed significant variations in performance based on the modifications made to the RAG settings. The adjustments in the number and size of chunks, chunk overlap ratios, and generation prompts had varying impacts on the systems' recall, faithfulness, noise sensitivity, and overall performance. These findings provide valuable insights into the behaviors of the retriever and generator components within the RAG systems, highlighting the importance of carefully tuning these settings to achieve optimal performance across different domains.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool],\n",
    "    'What is  averaged evaluation results for 8 RAG systems across 10 diverse domain datasets in the page 9',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"paper\"}\n",
      "=== Function Output ===\n",
      "The paper discusses the benchmark dataset used for evaluating Retrieval-Augmented Generation (RAG) systems, the process of curating questions from various open-domain question answering datasets, the generation of long-form answers using GPT-4, and the validation process to ensure no hallucinations were present. It also mentions downsampling in the Science and Biomedical domains for efficient evaluation. Additionally, the paper presents evaluation results for different RAG systems on datasets like ClapNQ, NovelQA, RobustQA - Writing, BioASQ, Finance, Lifestyle, Science, Technology, and Recreation, showcasing performance metrics for both retriever and generator components. Furthermore, it discusses the performance of RefChecker on the RefChecker benchmark using Llama 3 70B Instruct as both the extractor and checker, comparing the results with the best open-sourced combinations reported in the RefChecker paper.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool],\n",
    "    'What is  the summary of the paper',\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
